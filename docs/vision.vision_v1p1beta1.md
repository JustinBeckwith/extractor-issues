<!-- Do not edit this file. It is automatically generated by API Documenter. -->

[Home](./index.md) &gt; [@google/vision](./vision.md) &gt; [vision\_v1p1beta1](./vision.vision_v1p1beta1.md)

## vision\_v1p1beta1 namespace

<b>Signature:</b>

```typescript
export declare namespace vision_v1p1beta1 
```

## Classes

|  Class | Description |
|  --- | --- |
|  [Resource$Files](./vision.vision_v1p1beta1.resource_files.md) |  |
|  [Resource$Images](./vision.vision_v1p1beta1.resource_images.md) |  |
|  [Resource$Projects](./vision.vision_v1p1beta1.resource_projects.md) |  |
|  [Resource$Projects$Files](./vision.vision_v1p1beta1.resource_projects_files.md) |  |
|  [Resource$Projects$Images](./vision.vision_v1p1beta1.resource_projects_images.md) |  |
|  [Resource$Projects$Locations](./vision.vision_v1p1beta1.resource_projects_locations.md) |  |
|  [Resource$Projects$Locations$Files](./vision.vision_v1p1beta1.resource_projects_locations_files.md) |  |
|  [Resource$Projects$Locations$Images](./vision.vision_v1p1beta1.resource_projects_locations_images.md) |  |
|  [Vision](./vision.vision_v1p1beta1.vision.md) | Cloud Vision API<!-- -->Integrates Google Vision features, including image labeling, face, logo, and landmark detection, optical character recognition (OCR), and detection of explicit content, into applications. |

## Interfaces

|  Interface | Description |
|  --- | --- |
|  [Options](./vision.vision_v1p1beta1.options.md) |  |
|  [Params$Resource$Files$Annotate](./vision.vision_v1p1beta1.params_resource_files_annotate.md) |  |
|  [Params$Resource$Files$Asyncbatchannotate](./vision.vision_v1p1beta1.params_resource_files_asyncbatchannotate.md) |  |
|  [Params$Resource$Images$Annotate](./vision.vision_v1p1beta1.params_resource_images_annotate.md) |  |
|  [Params$Resource$Images$Asyncbatchannotate](./vision.vision_v1p1beta1.params_resource_images_asyncbatchannotate.md) |  |
|  [Params$Resource$Projects$Files$Annotate](./vision.vision_v1p1beta1.params_resource_projects_files_annotate.md) |  |
|  [Params$Resource$Projects$Files$Asyncbatchannotate](./vision.vision_v1p1beta1.params_resource_projects_files_asyncbatchannotate.md) |  |
|  [Params$Resource$Projects$Images$Annotate](./vision.vision_v1p1beta1.params_resource_projects_images_annotate.md) |  |
|  [Params$Resource$Projects$Images$Asyncbatchannotate](./vision.vision_v1p1beta1.params_resource_projects_images_asyncbatchannotate.md) |  |
|  [Params$Resource$Projects$Locations$Files$Annotate](./vision.vision_v1p1beta1.params_resource_projects_locations_files_annotate.md) |  |
|  [Params$Resource$Projects$Locations$Files$Asyncbatchannotate](./vision.vision_v1p1beta1.params_resource_projects_locations_files_asyncbatchannotate.md) |  |
|  [Params$Resource$Projects$Locations$Images$Annotate](./vision.vision_v1p1beta1.params_resource_projects_locations_images_annotate.md) |  |
|  [Params$Resource$Projects$Locations$Images$Asyncbatchannotate](./vision.vision_v1p1beta1.params_resource_projects_locations_images_asyncbatchannotate.md) |  |
|  [Schema$AnnotateFileResponse](./vision.vision_v1p1beta1.schema_annotatefileresponse.md) | Response to a single file annotation request. A file may contain one or more images, which individually have their own responses. |
|  [Schema$AnnotateImageResponse](./vision.vision_v1p1beta1.schema_annotateimageresponse.md) | Response to an image annotation request. |
|  [Schema$AsyncAnnotateFileResponse](./vision.vision_v1p1beta1.schema_asyncannotatefileresponse.md) | The response for a single offline file annotation request. |
|  [Schema$AsyncBatchAnnotateFilesResponse](./vision.vision_v1p1beta1.schema_asyncbatchannotatefilesresponse.md) | Response to an async batch file annotation request. |
|  [Schema$AsyncBatchAnnotateImagesResponse](./vision.vision_v1p1beta1.schema_asyncbatchannotateimagesresponse.md) | Response to an async batch image annotation request. |
|  [Schema$BatchAnnotateFilesResponse](./vision.vision_v1p1beta1.schema_batchannotatefilesresponse.md) | A list of file annotation responses. |
|  [Schema$BatchOperationMetadata](./vision.vision_v1p1beta1.schema_batchoperationmetadata.md) | Metadata for the batch operations such as the current state.<!-- -->This is included in the <code>metadata</code> field of the <code>Operation</code> returned by the <code>GetOperation</code> call of the <code>google::longrunning::Operations</code> service. |
|  [Schema$Block](./vision.vision_v1p1beta1.schema_block.md) | Logical element on the page. |
|  [Schema$BoundingPoly](./vision.vision_v1p1beta1.schema_boundingpoly.md) | A bounding polygon for the detected image annotation. |
|  [Schema$Color](./vision.vision_v1p1beta1.schema_color.md) | Represents a color in the RGBA color space. This representation is designed for simplicity of conversion to/from color representations in various languages over compactness; for example, the fields of this representation can be trivially provided to the constructor of "java.awt.Color" in Java; it can also be trivially provided to UIColor's "+colorWithRed:green:blue:alpha" method in iOS; and, with just a little work, it can be easily formatted into a CSS "rgba()" string in JavaScript, as well.<!-- -->Note: this proto does not carry information about the absolute color space that should be used to interpret the RGB value (e.g. sRGB, Adobe RGB, DCI-P3, BT.2020, etc.). By default, applications SHOULD assume the sRGB color space.<!-- -->Example (Java):<!-- -->import com.google.type.Color;<!-- -->// ... public static java.awt.Color fromProto(Color protocolor) { float alpha = protocolor.hasAlpha() ? protocolor.getAlpha().getValue() : 1.0;<!-- -->return new java.awt.Color( protocolor.getRed(), protocolor.getGreen(), protocolor.getBlue(), alpha); }<!-- -->public static Color toProto(java.awt.Color color) { float red = (float) color.getRed(); float green = (float) color.getGreen(); float blue = (float) color.getBlue(); float denominator = 255.0; Color.Builder resultBuilder = Color .newBuilder() .setRed(red / denominator) .setGreen(green / denominator) .setBlue(blue / denominator); int alpha = color.getAlpha(); if (alpha != 255) { result.setAlpha( FloatValue .newBuilder() .setValue(((float) alpha) / denominator) .build()); } return resultBuilder.build(); } // ...<!-- -->Example (iOS / Obj-C):<!-- -->// ... static UIColor\* fromProto(Color\* protocolor) { float red = \[protocolor red\]; float green = \[protocolor green\]; float blue = \[protocolor blue\]; FloatValue\* alpha\_wrapper = \[protocolor alpha\]; float alpha = 1.0; if (alpha\_wrapper != nil) { alpha = \[alpha\_wrapper value\]; } return \[UIColor colorWithRed:red green:green blue:blue alpha:alpha\]; }<!-- -->static Color\* toProto(UIColor\* color) { CGFloat red, green, blue, alpha; if (!\[color getRed:&amp;red green:&amp;green blue:&amp;blue alpha:&amp;alpha\]) { return nil; } Color\* result = \[\[Color alloc\] init\]; \[result setRed:red\]; \[result setGreen:green\]; \[result setBlue:blue\]; if (alpha &lt;<!-- -->= 0.9999) { \[result setAlpha:floatWrapperWithValue(alpha)\]; } \[result autorelease\]; return result; } // ...<!-- -->Example (JavaScript):<!-- -->// ...<!-- -->var protoToCssColor = function(rgb\_color) { var redFrac = rgb\_color.red \|\| 0.0; var greenFrac = rgb\_color.green \|\| 0.0; var blueFrac = rgb\_color.blue \|\| 0.0; var red = Math.floor(redFrac \* 255); var green = Math.floor(greenFrac \* 255); var blue = Math.floor(blueFrac \* 255);<!-- -->if (!('alpha' in rgb\_color)) { return rgbToCssColor\_(red, green, blue); }<!-- -->var alphaFrac = rgb\_color.alpha.value \|\| 0.0; var rgbParams = \[red, green, blue\].join(','); return \['rgba(', rgbParams, ',', alphaFrac, ')'\].join(''); }<!-- -->;<!-- -->var rgbToCssColor\_ = function(red, green, blue) { var rgbNumber = new Number((red &lt;<!-- -->&lt; 16) \| (green &lt;<!-- -->&lt; 8) \| blue); var hexString = rgbNumber.toString(16); var missingZeros = 6 - hexString.length; var resultBuilder = \['\#'\]; for (var i = 0; i &lt; missingZeros; i++) { resultBuilder.push('0'); } resultBuilder.push(hexString); return resultBuilder.join(''); }<!-- -->;<!-- -->// ... |
|  [Schema$ColorInfo](./vision.vision_v1p1beta1.schema_colorinfo.md) | Color information consists of RGB channels, score, and the fraction of the image that the color occupies in the image. |
|  [Schema$CropHint](./vision.vision_v1p1beta1.schema_crophint.md) | Single crop hint that is used to generate a new crop when serving an image. |
|  [Schema$CropHintsAnnotation](./vision.vision_v1p1beta1.schema_crophintsannotation.md) | Set of crop hints that are used to generate new crops when serving images. |
|  [Schema$DetectedBreak](./vision.vision_v1p1beta1.schema_detectedbreak.md) | Detected start or end of a structural component. |
|  [Schema$DetectedLanguage](./vision.vision_v1p1beta1.schema_detectedlanguage.md) | Detected language for a structural component. |
|  [Schema$DominantColorsAnnotation](./vision.vision_v1p1beta1.schema_dominantcolorsannotation.md) | Set of dominant colors and their corresponding scores. |
|  [Schema$EntityAnnotation](./vision.vision_v1p1beta1.schema_entityannotation.md) | Set of detected entity features. |
|  [Schema$FaceAnnotation](./vision.vision_v1p1beta1.schema_faceannotation.md) | A face annotation object contains the results of face detection. |
|  [Schema$GcsDestination](./vision.vision_v1p1beta1.schema_gcsdestination.md) | The Google Cloud Storage location where the output will be written to. |
|  [Schema$GcsSource](./vision.vision_v1p1beta1.schema_gcssource.md) | The Google Cloud Storage location where the input will be read from. |
|  [Schema$GoogleCloudVisionV1p1beta1AnnotateFileRequest](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1annotatefilerequest.md) | A request to annotate one single file, e.g. a PDF, TIFF or GIF file. |
|  [Schema$GoogleCloudVisionV1p1beta1AnnotateFileResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1annotatefileresponse.md) | Response to a single file annotation request. A file may contain one or more images, which individually have their own responses. |
|  [Schema$GoogleCloudVisionV1p1beta1AnnotateImageRequest](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1annotateimagerequest.md) | Request for performing Google Cloud Vision API tasks over a user-provided image, with user-requested features, and with context information. |
|  [Schema$GoogleCloudVisionV1p1beta1AnnotateImageResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1annotateimageresponse.md) | Response to an image annotation request. |
|  [Schema$GoogleCloudVisionV1p1beta1AsyncAnnotateFileRequest](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1asyncannotatefilerequest.md) | An offline file annotation request. |
|  [Schema$GoogleCloudVisionV1p1beta1AsyncAnnotateFileResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1asyncannotatefileresponse.md) | The response for a single offline file annotation request. |
|  [Schema$GoogleCloudVisionV1p1beta1AsyncBatchAnnotateFilesRequest](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1asyncbatchannotatefilesrequest.md) | Multiple async file annotation requests are batched into a single service call. |
|  [Schema$GoogleCloudVisionV1p1beta1AsyncBatchAnnotateFilesResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1asyncbatchannotatefilesresponse.md) | Response to an async batch file annotation request. |
|  [Schema$GoogleCloudVisionV1p1beta1AsyncBatchAnnotateImagesRequest](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1asyncbatchannotateimagesrequest.md) | Request for async image annotation for a list of images. |
|  [Schema$GoogleCloudVisionV1p1beta1BatchAnnotateFilesRequest](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1batchannotatefilesrequest.md) | A list of requests to annotate files using the BatchAnnotateFiles API. |
|  [Schema$GoogleCloudVisionV1p1beta1BatchAnnotateFilesResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1batchannotatefilesresponse.md) | A list of file annotation responses. |
|  [Schema$GoogleCloudVisionV1p1beta1BatchAnnotateImagesRequest](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1batchannotateimagesrequest.md) | Multiple image annotation requests are batched into a single service call. |
|  [Schema$GoogleCloudVisionV1p1beta1BatchAnnotateImagesResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1batchannotateimagesresponse.md) | Response to a batch image annotation request. |
|  [Schema$GoogleCloudVisionV1p1beta1Block](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1block.md) | Logical element on the page. |
|  [Schema$GoogleCloudVisionV1p1beta1BoundingPoly](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1boundingpoly.md) | A bounding polygon for the detected image annotation. |
|  [Schema$GoogleCloudVisionV1p1beta1ColorInfo](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1colorinfo.md) | Color information consists of RGB channels, score, and the fraction of the image that the color occupies in the image. |
|  [Schema$GoogleCloudVisionV1p1beta1CropHint](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1crophint.md) | Single crop hint that is used to generate a new crop when serving an image. |
|  [Schema$GoogleCloudVisionV1p1beta1CropHintsAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1crophintsannotation.md) | Set of crop hints that are used to generate new crops when serving images. |
|  [Schema$GoogleCloudVisionV1p1beta1CropHintsParams](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1crophintsparams.md) | Parameters for crop hints annotation request. |
|  [Schema$GoogleCloudVisionV1p1beta1DominantColorsAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1dominantcolorsannotation.md) | Set of dominant colors and their corresponding scores. |
|  [Schema$GoogleCloudVisionV1p1beta1EntityAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1entityannotation.md) | Set of detected entity features. |
|  [Schema$GoogleCloudVisionV1p1beta1FaceAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1faceannotation.md) | A face annotation object contains the results of face detection. |
|  [Schema$GoogleCloudVisionV1p1beta1FaceAnnotationLandmark](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1faceannotationlandmark.md) | A face-specific landmark (for example, a face feature). |
|  [Schema$GoogleCloudVisionV1p1beta1Feature](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1feature.md) | The type of Google Cloud Vision API detection to perform, and the maximum number of results to return for that type. Multiple <code>Feature</code> objects can be specified in the <code>features</code> list. |
|  [Schema$GoogleCloudVisionV1p1beta1GcsDestination](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1gcsdestination.md) | The Google Cloud Storage location where the output will be written to. |
|  [Schema$GoogleCloudVisionV1p1beta1GcsSource](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1gcssource.md) | The Google Cloud Storage location where the input will be read from. |
|  [Schema$GoogleCloudVisionV1p1beta1Image](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1image.md) | Client image to perform Google Cloud Vision API tasks over. |
|  [Schema$GoogleCloudVisionV1p1beta1ImageAnnotationContext](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1imageannotationcontext.md) | If an image was produced from a file (e.g. a PDF), this message gives information about the source of that image. |
|  [Schema$GoogleCloudVisionV1p1beta1ImageContext](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1imagecontext.md) | Image context and/or feature-specific parameters. |
|  [Schema$GoogleCloudVisionV1p1beta1ImageProperties](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1imageproperties.md) | Stores image properties, such as dominant colors. |
|  [Schema$GoogleCloudVisionV1p1beta1ImageSource](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1imagesource.md) | External image source (Google Cloud Storage or web URL image location). |
|  [Schema$GoogleCloudVisionV1p1beta1InputConfig](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1inputconfig.md) | The desired input location and metadata. |
|  [Schema$GoogleCloudVisionV1p1beta1LatLongRect](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1latlongrect.md) | Rectangle determined by min and max <code>LatLng</code> pairs. |
|  [Schema$GoogleCloudVisionV1p1beta1LocalizedObjectAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1localizedobjectannotation.md) | Set of detected objects with bounding boxes. |
|  [Schema$GoogleCloudVisionV1p1beta1LocationInfo](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1locationinfo.md) | Detected entity location information. |
|  [Schema$GoogleCloudVisionV1p1beta1NormalizedVertex](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1normalizedvertex.md) | A vertex represents a 2D point in the image. NOTE: the normalized vertex coordinates are relative to the original image and range from 0 to 1. |
|  [Schema$GoogleCloudVisionV1p1beta1OperationMetadata](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1operationmetadata.md) | Contains metadata for the BatchAnnotateImages operation. |
|  [Schema$GoogleCloudVisionV1p1beta1OutputConfig](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1outputconfig.md) | The desired output location and metadata. |
|  [Schema$GoogleCloudVisionV1p1beta1Page](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1page.md) | Detected page from OCR. |
|  [Schema$GoogleCloudVisionV1p1beta1Paragraph](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1paragraph.md) | Structural unit of text representing a number of words in certain order. |
|  [Schema$GoogleCloudVisionV1p1beta1Position](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1position.md) | A 3D position in the image, used primarily for Face detection landmarks. A valid Position must have both x and y coordinates. The position coordinates are in the same scale as the original image. |
|  [Schema$GoogleCloudVisionV1p1beta1Product](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1product.md) | A Product contains ReferenceImages. |
|  [Schema$GoogleCloudVisionV1p1beta1ProductKeyValue](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1productkeyvalue.md) | A product label represented as a key-value pair. |
|  [Schema$GoogleCloudVisionV1p1beta1ProductSearchParams](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1productsearchparams.md) | Parameters for a product search request. |
|  [Schema$GoogleCloudVisionV1p1beta1ProductSearchResults](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1productsearchresults.md) | Results for a product search request. |
|  [Schema$GoogleCloudVisionV1p1beta1ProductSearchResultsGroupedResult](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1productsearchresultsgroupedresult.md) | Information about the products similar to a single product in a query image. |
|  [Schema$GoogleCloudVisionV1p1beta1ProductSearchResultsObjectAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1productsearchresultsobjectannotation.md) | Prediction for what the object in the bounding box is. |
|  [Schema$GoogleCloudVisionV1p1beta1ProductSearchResultsResult](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1productsearchresultsresult.md) | Information about a product. |
|  [Schema$GoogleCloudVisionV1p1beta1Property](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1property.md) | A <code>Property</code> consists of a user-supplied name/value pair. |
|  [Schema$GoogleCloudVisionV1p1beta1SafeSearchAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1safesearchannotation.md) | Set of features pertaining to the image, computed by computer vision methods over safe-search verticals (for example, adult, spoof, medical, violence). |
|  [Schema$GoogleCloudVisionV1p1beta1Symbol](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1symbol.md) | A single symbol representation. |
|  [Schema$GoogleCloudVisionV1p1beta1TextAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1textannotation.md) | TextAnnotation contains a structured representation of OCR extracted text. The hierarchy of an OCR extracted text structure is like this: TextAnnotation -<!-- -->&gt; Page -<!-- -->&gt; Block -<!-- -->&gt; Paragraph -<!-- -->&gt; Word -<!-- -->&gt; Symbol Each structural component, starting from Page, may further have their own properties. Properties describe detected languages, breaks etc.. Please refer to the TextAnnotation.TextProperty message definition below for more detail. |
|  [Schema$GoogleCloudVisionV1p1beta1TextAnnotationDetectedBreak](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1textannotationdetectedbreak.md) | Detected start or end of a structural component. |
|  [Schema$GoogleCloudVisionV1p1beta1TextAnnotationDetectedLanguage](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1textannotationdetectedlanguage.md) | Detected language for a structural component. |
|  [Schema$GoogleCloudVisionV1p1beta1TextAnnotationTextProperty](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1textannotationtextproperty.md) | Additional information detected on the structural component. |
|  [Schema$GoogleCloudVisionV1p1beta1Vertex](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1vertex.md) | A vertex represents a 2D point in the image. NOTE: the vertex coordinates are in the same scale as the original image. |
|  [Schema$GoogleCloudVisionV1p1beta1WebDetection](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1webdetection.md) | Relevant information for the image from the Internet. |
|  [Schema$GoogleCloudVisionV1p1beta1WebDetectionParams](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1webdetectionparams.md) | Parameters for web detection request. |
|  [Schema$GoogleCloudVisionV1p1beta1WebDetectionWebEntity](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1webdetectionwebentity.md) | Entity deduced from similar images on the Internet. |
|  [Schema$GoogleCloudVisionV1p1beta1WebDetectionWebImage](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1webdetectionwebimage.md) | Metadata for online images. |
|  [Schema$GoogleCloudVisionV1p1beta1WebDetectionWebLabel](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1webdetectionweblabel.md) | Label to provide extra metadata for the web detection. |
|  [Schema$GoogleCloudVisionV1p1beta1WebDetectionWebPage](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1webdetectionwebpage.md) | Metadata for web pages. |
|  [Schema$GoogleCloudVisionV1p1beta1Word](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p1beta1word.md) | A word representation. |
|  [Schema$GoogleCloudVisionV1p2beta1AnnotateFileResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1annotatefileresponse.md) | Response to a single file annotation request. A file may contain one or more images, which individually have their own responses. |
|  [Schema$GoogleCloudVisionV1p2beta1AnnotateImageResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1annotateimageresponse.md) | Response to an image annotation request. |
|  [Schema$GoogleCloudVisionV1p2beta1AsyncAnnotateFileResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1asyncannotatefileresponse.md) | The response for a single offline file annotation request. |
|  [Schema$GoogleCloudVisionV1p2beta1AsyncBatchAnnotateFilesResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1asyncbatchannotatefilesresponse.md) | Response to an async batch file annotation request. |
|  [Schema$GoogleCloudVisionV1p2beta1Block](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1block.md) | Logical element on the page. |
|  [Schema$GoogleCloudVisionV1p2beta1BoundingPoly](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1boundingpoly.md) | A bounding polygon for the detected image annotation. |
|  [Schema$GoogleCloudVisionV1p2beta1ColorInfo](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1colorinfo.md) | Color information consists of RGB channels, score, and the fraction of the image that the color occupies in the image. |
|  [Schema$GoogleCloudVisionV1p2beta1CropHint](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1crophint.md) | Single crop hint that is used to generate a new crop when serving an image. |
|  [Schema$GoogleCloudVisionV1p2beta1CropHintsAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1crophintsannotation.md) | Set of crop hints that are used to generate new crops when serving images. |
|  [Schema$GoogleCloudVisionV1p2beta1DominantColorsAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1dominantcolorsannotation.md) | Set of dominant colors and their corresponding scores. |
|  [Schema$GoogleCloudVisionV1p2beta1EntityAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1entityannotation.md) | Set of detected entity features. |
|  [Schema$GoogleCloudVisionV1p2beta1FaceAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1faceannotation.md) | A face annotation object contains the results of face detection. |
|  [Schema$GoogleCloudVisionV1p2beta1FaceAnnotationLandmark](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1faceannotationlandmark.md) | A face-specific landmark (for example, a face feature). |
|  [Schema$GoogleCloudVisionV1p2beta1GcsDestination](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1gcsdestination.md) | The Google Cloud Storage location where the output will be written to. |
|  [Schema$GoogleCloudVisionV1p2beta1GcsSource](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1gcssource.md) | The Google Cloud Storage location where the input will be read from. |
|  [Schema$GoogleCloudVisionV1p2beta1ImageAnnotationContext](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1imageannotationcontext.md) | If an image was produced from a file (e.g. a PDF), this message gives information about the source of that image. |
|  [Schema$GoogleCloudVisionV1p2beta1ImageProperties](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1imageproperties.md) | Stores image properties, such as dominant colors. |
|  [Schema$GoogleCloudVisionV1p2beta1InputConfig](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1inputconfig.md) | The desired input location and metadata. |
|  [Schema$GoogleCloudVisionV1p2beta1LocalizedObjectAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1localizedobjectannotation.md) | Set of detected objects with bounding boxes. |
|  [Schema$GoogleCloudVisionV1p2beta1LocationInfo](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1locationinfo.md) | Detected entity location information. |
|  [Schema$GoogleCloudVisionV1p2beta1NormalizedVertex](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1normalizedvertex.md) | A vertex represents a 2D point in the image. NOTE: the normalized vertex coordinates are relative to the original image and range from 0 to 1. |
|  [Schema$GoogleCloudVisionV1p2beta1OperationMetadata](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1operationmetadata.md) | Contains metadata for the BatchAnnotateImages operation. |
|  [Schema$GoogleCloudVisionV1p2beta1OutputConfig](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1outputconfig.md) | The desired output location and metadata. |
|  [Schema$GoogleCloudVisionV1p2beta1Page](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1page.md) | Detected page from OCR. |
|  [Schema$GoogleCloudVisionV1p2beta1Paragraph](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1paragraph.md) | Structural unit of text representing a number of words in certain order. |
|  [Schema$GoogleCloudVisionV1p2beta1Position](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1position.md) | A 3D position in the image, used primarily for Face detection landmarks. A valid Position must have both x and y coordinates. The position coordinates are in the same scale as the original image. |
|  [Schema$GoogleCloudVisionV1p2beta1Product](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1product.md) | A Product contains ReferenceImages. |
|  [Schema$GoogleCloudVisionV1p2beta1ProductKeyValue](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1productkeyvalue.md) | A product label represented as a key-value pair. |
|  [Schema$GoogleCloudVisionV1p2beta1ProductSearchResults](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1productsearchresults.md) | Results for a product search request. |
|  [Schema$GoogleCloudVisionV1p2beta1ProductSearchResultsGroupedResult](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1productsearchresultsgroupedresult.md) | Information about the products similar to a single product in a query image. |
|  [Schema$GoogleCloudVisionV1p2beta1ProductSearchResultsObjectAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1productsearchresultsobjectannotation.md) | Prediction for what the object in the bounding box is. |
|  [Schema$GoogleCloudVisionV1p2beta1ProductSearchResultsResult](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1productsearchresultsresult.md) | Information about a product. |
|  [Schema$GoogleCloudVisionV1p2beta1Property](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1property.md) | A <code>Property</code> consists of a user-supplied name/value pair. |
|  [Schema$GoogleCloudVisionV1p2beta1SafeSearchAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1safesearchannotation.md) | Set of features pertaining to the image, computed by computer vision methods over safe-search verticals (for example, adult, spoof, medical, violence). |
|  [Schema$GoogleCloudVisionV1p2beta1Symbol](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1symbol.md) | A single symbol representation. |
|  [Schema$GoogleCloudVisionV1p2beta1TextAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1textannotation.md) | TextAnnotation contains a structured representation of OCR extracted text. The hierarchy of an OCR extracted text structure is like this: TextAnnotation -<!-- -->&gt; Page -<!-- -->&gt; Block -<!-- -->&gt; Paragraph -<!-- -->&gt; Word -<!-- -->&gt; Symbol Each structural component, starting from Page, may further have their own properties. Properties describe detected languages, breaks etc.. Please refer to the TextAnnotation.TextProperty message definition below for more detail. |
|  [Schema$GoogleCloudVisionV1p2beta1TextAnnotationDetectedBreak](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1textannotationdetectedbreak.md) | Detected start or end of a structural component. |
|  [Schema$GoogleCloudVisionV1p2beta1TextAnnotationDetectedLanguage](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1textannotationdetectedlanguage.md) | Detected language for a structural component. |
|  [Schema$GoogleCloudVisionV1p2beta1TextAnnotationTextProperty](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1textannotationtextproperty.md) | Additional information detected on the structural component. |
|  [Schema$GoogleCloudVisionV1p2beta1Vertex](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1vertex.md) | A vertex represents a 2D point in the image. NOTE: the vertex coordinates are in the same scale as the original image. |
|  [Schema$GoogleCloudVisionV1p2beta1WebDetection](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1webdetection.md) | Relevant information for the image from the Internet. |
|  [Schema$GoogleCloudVisionV1p2beta1WebDetectionWebEntity](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1webdetectionwebentity.md) | Entity deduced from similar images on the Internet. |
|  [Schema$GoogleCloudVisionV1p2beta1WebDetectionWebImage](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1webdetectionwebimage.md) | Metadata for online images. |
|  [Schema$GoogleCloudVisionV1p2beta1WebDetectionWebLabel](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1webdetectionweblabel.md) | Label to provide extra metadata for the web detection. |
|  [Schema$GoogleCloudVisionV1p2beta1WebDetectionWebPage](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1webdetectionwebpage.md) | Metadata for web pages. |
|  [Schema$GoogleCloudVisionV1p2beta1Word](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p2beta1word.md) | A word representation. |
|  [Schema$GoogleCloudVisionV1p3beta1AnnotateFileResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1annotatefileresponse.md) | Response to a single file annotation request. A file may contain one or more images, which individually have their own responses. |
|  [Schema$GoogleCloudVisionV1p3beta1AnnotateImageResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1annotateimageresponse.md) | Response to an image annotation request. |
|  [Schema$GoogleCloudVisionV1p3beta1AsyncAnnotateFileResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1asyncannotatefileresponse.md) | The response for a single offline file annotation request. |
|  [Schema$GoogleCloudVisionV1p3beta1AsyncBatchAnnotateFilesResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1asyncbatchannotatefilesresponse.md) | Response to an async batch file annotation request. |
|  [Schema$GoogleCloudVisionV1p3beta1BatchOperationMetadata](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1batchoperationmetadata.md) | Metadata for the batch operations such as the current state.<!-- -->This is included in the <code>metadata</code> field of the <code>Operation</code> returned by the <code>GetOperation</code> call of the <code>google::longrunning::Operations</code> service. |
|  [Schema$GoogleCloudVisionV1p3beta1Block](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1block.md) | Logical element on the page. |
|  [Schema$GoogleCloudVisionV1p3beta1BoundingPoly](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1boundingpoly.md) | A bounding polygon for the detected image annotation. |
|  [Schema$GoogleCloudVisionV1p3beta1ColorInfo](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1colorinfo.md) | Color information consists of RGB channels, score, and the fraction of the image that the color occupies in the image. |
|  [Schema$GoogleCloudVisionV1p3beta1CropHint](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1crophint.md) | Single crop hint that is used to generate a new crop when serving an image. |
|  [Schema$GoogleCloudVisionV1p3beta1CropHintsAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1crophintsannotation.md) | Set of crop hints that are used to generate new crops when serving images. |
|  [Schema$GoogleCloudVisionV1p3beta1DominantColorsAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1dominantcolorsannotation.md) | Set of dominant colors and their corresponding scores. |
|  [Schema$GoogleCloudVisionV1p3beta1EntityAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1entityannotation.md) | Set of detected entity features. |
|  [Schema$GoogleCloudVisionV1p3beta1FaceAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1faceannotation.md) | A face annotation object contains the results of face detection. |
|  [Schema$GoogleCloudVisionV1p3beta1FaceAnnotationLandmark](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1faceannotationlandmark.md) | A face-specific landmark (for example, a face feature). |
|  [Schema$GoogleCloudVisionV1p3beta1GcsDestination](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1gcsdestination.md) | The Google Cloud Storage location where the output will be written to. |
|  [Schema$GoogleCloudVisionV1p3beta1GcsSource](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1gcssource.md) | The Google Cloud Storage location where the input will be read from. |
|  [Schema$GoogleCloudVisionV1p3beta1ImageAnnotationContext](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1imageannotationcontext.md) | If an image was produced from a file (e.g. a PDF), this message gives information about the source of that image. |
|  [Schema$GoogleCloudVisionV1p3beta1ImageProperties](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1imageproperties.md) | Stores image properties, such as dominant colors. |
|  [Schema$GoogleCloudVisionV1p3beta1ImportProductSetsResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1importproductsetsresponse.md) | Response message for the <code>ImportProductSets</code> method.<!-- -->This message is returned by the google.longrunning.Operations.GetOperation method in the returned google.longrunning.Operation.response field. |
|  [Schema$GoogleCloudVisionV1p3beta1InputConfig](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1inputconfig.md) | The desired input location and metadata. |
|  [Schema$GoogleCloudVisionV1p3beta1LocalizedObjectAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1localizedobjectannotation.md) | Set of detected objects with bounding boxes. |
|  [Schema$GoogleCloudVisionV1p3beta1LocationInfo](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1locationinfo.md) | Detected entity location information. |
|  [Schema$GoogleCloudVisionV1p3beta1NormalizedVertex](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1normalizedvertex.md) | A vertex represents a 2D point in the image. NOTE: the normalized vertex coordinates are relative to the original image and range from 0 to 1. |
|  [Schema$GoogleCloudVisionV1p3beta1OperationMetadata](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1operationmetadata.md) | Contains metadata for the BatchAnnotateImages operation. |
|  [Schema$GoogleCloudVisionV1p3beta1OutputConfig](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1outputconfig.md) | The desired output location and metadata. |
|  [Schema$GoogleCloudVisionV1p3beta1Page](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1page.md) | Detected page from OCR. |
|  [Schema$GoogleCloudVisionV1p3beta1Paragraph](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1paragraph.md) | Structural unit of text representing a number of words in certain order. |
|  [Schema$GoogleCloudVisionV1p3beta1Position](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1position.md) | A 3D position in the image, used primarily for Face detection landmarks. A valid Position must have both x and y coordinates. The position coordinates are in the same scale as the original image. |
|  [Schema$GoogleCloudVisionV1p3beta1Product](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1product.md) | A Product contains ReferenceImages. |
|  [Schema$GoogleCloudVisionV1p3beta1ProductKeyValue](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1productkeyvalue.md) | A product label represented as a key-value pair. |
|  [Schema$GoogleCloudVisionV1p3beta1ProductSearchResults](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1productsearchresults.md) | Results for a product search request. |
|  [Schema$GoogleCloudVisionV1p3beta1ProductSearchResultsGroupedResult](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1productsearchresultsgroupedresult.md) | Information about the products similar to a single product in a query image. |
|  [Schema$GoogleCloudVisionV1p3beta1ProductSearchResultsObjectAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1productsearchresultsobjectannotation.md) | Prediction for what the object in the bounding box is. |
|  [Schema$GoogleCloudVisionV1p3beta1ProductSearchResultsResult](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1productsearchresultsresult.md) | Information about a product. |
|  [Schema$GoogleCloudVisionV1p3beta1Property](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1property.md) | A <code>Property</code> consists of a user-supplied name/value pair. |
|  [Schema$GoogleCloudVisionV1p3beta1ReferenceImage](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1referenceimage.md) | A <code>ReferenceImage</code> represents a product image and its associated metadata, such as bounding boxes. |
|  [Schema$GoogleCloudVisionV1p3beta1SafeSearchAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1safesearchannotation.md) | Set of features pertaining to the image, computed by computer vision methods over safe-search verticals (for example, adult, spoof, medical, violence). |
|  [Schema$GoogleCloudVisionV1p3beta1Symbol](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1symbol.md) | A single symbol representation. |
|  [Schema$GoogleCloudVisionV1p3beta1TextAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1textannotation.md) | TextAnnotation contains a structured representation of OCR extracted text. The hierarchy of an OCR extracted text structure is like this: TextAnnotation -<!-- -->&gt; Page -<!-- -->&gt; Block -<!-- -->&gt; Paragraph -<!-- -->&gt; Word -<!-- -->&gt; Symbol Each structural component, starting from Page, may further have their own properties. Properties describe detected languages, breaks etc.. Please refer to the TextAnnotation.TextProperty message definition below for more detail. |
|  [Schema$GoogleCloudVisionV1p3beta1TextAnnotationDetectedBreak](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1textannotationdetectedbreak.md) | Detected start or end of a structural component. |
|  [Schema$GoogleCloudVisionV1p3beta1TextAnnotationDetectedLanguage](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1textannotationdetectedlanguage.md) | Detected language for a structural component. |
|  [Schema$GoogleCloudVisionV1p3beta1TextAnnotationTextProperty](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1textannotationtextproperty.md) | Additional information detected on the structural component. |
|  [Schema$GoogleCloudVisionV1p3beta1Vertex](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1vertex.md) | A vertex represents a 2D point in the image. NOTE: the vertex coordinates are in the same scale as the original image. |
|  [Schema$GoogleCloudVisionV1p3beta1WebDetection](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1webdetection.md) | Relevant information for the image from the Internet. |
|  [Schema$GoogleCloudVisionV1p3beta1WebDetectionWebEntity](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1webdetectionwebentity.md) | Entity deduced from similar images on the Internet. |
|  [Schema$GoogleCloudVisionV1p3beta1WebDetectionWebImage](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1webdetectionwebimage.md) | Metadata for online images. |
|  [Schema$GoogleCloudVisionV1p3beta1WebDetectionWebLabel](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1webdetectionweblabel.md) | Label to provide extra metadata for the web detection. |
|  [Schema$GoogleCloudVisionV1p3beta1WebDetectionWebPage](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1webdetectionwebpage.md) | Metadata for web pages. |
|  [Schema$GoogleCloudVisionV1p3beta1Word](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p3beta1word.md) | A word representation. |
|  [Schema$GoogleCloudVisionV1p4beta1AnnotateFileResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1annotatefileresponse.md) | Response to a single file annotation request. A file may contain one or more images, which individually have their own responses. |
|  [Schema$GoogleCloudVisionV1p4beta1AnnotateImageResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1annotateimageresponse.md) | Response to an image annotation request. |
|  [Schema$GoogleCloudVisionV1p4beta1AsyncAnnotateFileResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1asyncannotatefileresponse.md) | The response for a single offline file annotation request. |
|  [Schema$GoogleCloudVisionV1p4beta1AsyncBatchAnnotateFilesResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1asyncbatchannotatefilesresponse.md) | Response to an async batch file annotation request. |
|  [Schema$GoogleCloudVisionV1p4beta1AsyncBatchAnnotateImagesResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1asyncbatchannotateimagesresponse.md) | Response to an async batch image annotation request. |
|  [Schema$GoogleCloudVisionV1p4beta1BatchAnnotateFilesResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1batchannotatefilesresponse.md) | A list of file annotation responses. |
|  [Schema$GoogleCloudVisionV1p4beta1BatchOperationMetadata](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1batchoperationmetadata.md) | Metadata for the batch operations such as the current state.<!-- -->This is included in the <code>metadata</code> field of the <code>Operation</code> returned by the <code>GetOperation</code> call of the <code>google::longrunning::Operations</code> service. |
|  [Schema$GoogleCloudVisionV1p4beta1Block](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1block.md) | Logical element on the page. |
|  [Schema$GoogleCloudVisionV1p4beta1BoundingPoly](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1boundingpoly.md) | A bounding polygon for the detected image annotation. |
|  [Schema$GoogleCloudVisionV1p4beta1Celebrity](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1celebrity.md) | A Celebrity is a group of Faces with an identity. |
|  [Schema$GoogleCloudVisionV1p4beta1ColorInfo](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1colorinfo.md) | Color information consists of RGB channels, score, and the fraction of the image that the color occupies in the image. |
|  [Schema$GoogleCloudVisionV1p4beta1CropHint](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1crophint.md) | Single crop hint that is used to generate a new crop when serving an image. |
|  [Schema$GoogleCloudVisionV1p4beta1CropHintsAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1crophintsannotation.md) | Set of crop hints that are used to generate new crops when serving images. |
|  [Schema$GoogleCloudVisionV1p4beta1DominantColorsAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1dominantcolorsannotation.md) | Set of dominant colors and their corresponding scores. |
|  [Schema$GoogleCloudVisionV1p4beta1EntityAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1entityannotation.md) | Set of detected entity features. |
|  [Schema$GoogleCloudVisionV1p4beta1FaceAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1faceannotation.md) | A face annotation object contains the results of face detection. |
|  [Schema$GoogleCloudVisionV1p4beta1FaceAnnotationLandmark](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1faceannotationlandmark.md) | A face-specific landmark (for example, a face feature). |
|  [Schema$GoogleCloudVisionV1p4beta1FaceRecognitionResult](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1facerecognitionresult.md) | Information about a face's identity. |
|  [Schema$GoogleCloudVisionV1p4beta1GcsDestination](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1gcsdestination.md) | The Google Cloud Storage location where the output will be written to. |
|  [Schema$GoogleCloudVisionV1p4beta1GcsSource](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1gcssource.md) | The Google Cloud Storage location where the input will be read from. |
|  [Schema$GoogleCloudVisionV1p4beta1ImageAnnotationContext](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1imageannotationcontext.md) | If an image was produced from a file (e.g. a PDF), this message gives information about the source of that image. |
|  [Schema$GoogleCloudVisionV1p4beta1ImageProperties](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1imageproperties.md) | Stores image properties, such as dominant colors. |
|  [Schema$GoogleCloudVisionV1p4beta1ImportProductSetsResponse](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1importproductsetsresponse.md) | Response message for the <code>ImportProductSets</code> method.<!-- -->This message is returned by the google.longrunning.Operations.GetOperation method in the returned google.longrunning.Operation.response field. |
|  [Schema$GoogleCloudVisionV1p4beta1InputConfig](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1inputconfig.md) | The desired input location and metadata. |
|  [Schema$GoogleCloudVisionV1p4beta1LocalizedObjectAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1localizedobjectannotation.md) | Set of detected objects with bounding boxes. |
|  [Schema$GoogleCloudVisionV1p4beta1LocationInfo](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1locationinfo.md) | Detected entity location information. |
|  [Schema$GoogleCloudVisionV1p4beta1NormalizedVertex](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1normalizedvertex.md) | A vertex represents a 2D point in the image. NOTE: the normalized vertex coordinates are relative to the original image and range from 0 to 1. |
|  [Schema$GoogleCloudVisionV1p4beta1OperationMetadata](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1operationmetadata.md) | Contains metadata for the BatchAnnotateImages operation. |
|  [Schema$GoogleCloudVisionV1p4beta1OutputConfig](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1outputconfig.md) | The desired output location and metadata. |
|  [Schema$GoogleCloudVisionV1p4beta1Page](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1page.md) | Detected page from OCR. |
|  [Schema$GoogleCloudVisionV1p4beta1Paragraph](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1paragraph.md) | Structural unit of text representing a number of words in certain order. |
|  [Schema$GoogleCloudVisionV1p4beta1Position](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1position.md) | A 3D position in the image, used primarily for Face detection landmarks. A valid Position must have both x and y coordinates. The position coordinates are in the same scale as the original image. |
|  [Schema$GoogleCloudVisionV1p4beta1Product](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1product.md) | A Product contains ReferenceImages. |
|  [Schema$GoogleCloudVisionV1p4beta1ProductKeyValue](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1productkeyvalue.md) | A product label represented as a key-value pair. |
|  [Schema$GoogleCloudVisionV1p4beta1ProductSearchResults](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1productsearchresults.md) | Results for a product search request. |
|  [Schema$GoogleCloudVisionV1p4beta1ProductSearchResultsGroupedResult](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1productsearchresultsgroupedresult.md) | Information about the products similar to a single product in a query image. |
|  [Schema$GoogleCloudVisionV1p4beta1ProductSearchResultsObjectAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1productsearchresultsobjectannotation.md) | Prediction for what the object in the bounding box is. |
|  [Schema$GoogleCloudVisionV1p4beta1ProductSearchResultsResult](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1productsearchresultsresult.md) | Information about a product. |
|  [Schema$GoogleCloudVisionV1p4beta1Property](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1property.md) | A <code>Property</code> consists of a user-supplied name/value pair. |
|  [Schema$GoogleCloudVisionV1p4beta1ReferenceImage](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1referenceimage.md) | A <code>ReferenceImage</code> represents a product image and its associated metadata, such as bounding boxes. |
|  [Schema$GoogleCloudVisionV1p4beta1SafeSearchAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1safesearchannotation.md) | Set of features pertaining to the image, computed by computer vision methods over safe-search verticals (for example, adult, spoof, medical, violence). |
|  [Schema$GoogleCloudVisionV1p4beta1Symbol](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1symbol.md) | A single symbol representation. |
|  [Schema$GoogleCloudVisionV1p4beta1TextAnnotation](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1textannotation.md) | TextAnnotation contains a structured representation of OCR extracted text. The hierarchy of an OCR extracted text structure is like this: TextAnnotation -<!-- -->&gt; Page -<!-- -->&gt; Block -<!-- -->&gt; Paragraph -<!-- -->&gt; Word -<!-- -->&gt; Symbol Each structural component, starting from Page, may further have their own properties. Properties describe detected languages, breaks etc.. Please refer to the TextAnnotation.TextProperty message definition below for more detail. |
|  [Schema$GoogleCloudVisionV1p4beta1TextAnnotationDetectedBreak](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1textannotationdetectedbreak.md) | Detected start or end of a structural component. |
|  [Schema$GoogleCloudVisionV1p4beta1TextAnnotationDetectedLanguage](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1textannotationdetectedlanguage.md) | Detected language for a structural component. |
|  [Schema$GoogleCloudVisionV1p4beta1TextAnnotationTextProperty](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1textannotationtextproperty.md) | Additional information detected on the structural component. |
|  [Schema$GoogleCloudVisionV1p4beta1Vertex](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1vertex.md) | A vertex represents a 2D point in the image. NOTE: the vertex coordinates are in the same scale as the original image. |
|  [Schema$GoogleCloudVisionV1p4beta1WebDetection](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1webdetection.md) | Relevant information for the image from the Internet. |
|  [Schema$GoogleCloudVisionV1p4beta1WebDetectionWebEntity](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1webdetectionwebentity.md) | Entity deduced from similar images on the Internet. |
|  [Schema$GoogleCloudVisionV1p4beta1WebDetectionWebImage](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1webdetectionwebimage.md) | Metadata for online images. |
|  [Schema$GoogleCloudVisionV1p4beta1WebDetectionWebLabel](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1webdetectionweblabel.md) | Label to provide extra metadata for the web detection. |
|  [Schema$GoogleCloudVisionV1p4beta1WebDetectionWebPage](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1webdetectionwebpage.md) | Metadata for web pages. |
|  [Schema$GoogleCloudVisionV1p4beta1Word](./vision.vision_v1p1beta1.schema_googlecloudvisionv1p4beta1word.md) | A word representation. |
|  [Schema$GroupedResult](./vision.vision_v1p1beta1.schema_groupedresult.md) | Information about the products similar to a single product in a query image. |
|  [Schema$ImageAnnotationContext](./vision.vision_v1p1beta1.schema_imageannotationcontext.md) | If an image was produced from a file (e.g. a PDF), this message gives information about the source of that image. |
|  [Schema$ImageProperties](./vision.vision_v1p1beta1.schema_imageproperties.md) | Stores image properties, such as dominant colors. |
|  [Schema$ImportProductSetsResponse](./vision.vision_v1p1beta1.schema_importproductsetsresponse.md) | Response message for the <code>ImportProductSets</code> method.<!-- -->This message is returned by the google.longrunning.Operations.GetOperation method in the returned google.longrunning.Operation.response field. |
|  [Schema$InputConfig](./vision.vision_v1p1beta1.schema_inputconfig.md) | The desired input location and metadata. |
|  [Schema$KeyValue](./vision.vision_v1p1beta1.schema_keyvalue.md) | A product label represented as a key-value pair. |
|  [Schema$Landmark](./vision.vision_v1p1beta1.schema_landmark.md) | A face-specific landmark (for example, a face feature). |
|  [Schema$LatLng](./vision.vision_v1p1beta1.schema_latlng.md) | An object representing a latitude/longitude pair. This is expressed as a pair of doubles representing degrees latitude and degrees longitude. Unless specified otherwise, this must conform to the &lt;<!-- -->a href="http://www.unoosa.org/pdf/icg/2012/template/WGS\_84.pdf"<!-- -->&gt;<!-- -->WGS84 standard<!-- -->&lt;<!-- -->/a<!-- -->&gt;<!-- -->. Values must be within normalized ranges. |
|  [Schema$LocalizedObjectAnnotation](./vision.vision_v1p1beta1.schema_localizedobjectannotation.md) | Set of detected objects with bounding boxes. |
|  [Schema$LocationInfo](./vision.vision_v1p1beta1.schema_locationinfo.md) | Detected entity location information. |
|  [Schema$NormalizedVertex](./vision.vision_v1p1beta1.schema_normalizedvertex.md) | A vertex represents a 2D point in the image. NOTE: the normalized vertex coordinates are relative to the original image and range from 0 to 1. |
|  [Schema$ObjectAnnotation](./vision.vision_v1p1beta1.schema_objectannotation.md) | Prediction for what the object in the bounding box is. |
|  [Schema$Operation](./vision.vision_v1p1beta1.schema_operation.md) | This resource represents a long-running operation that is the result of a network API call. |
|  [Schema$OperationMetadata](./vision.vision_v1p1beta1.schema_operationmetadata.md) | Contains metadata for the BatchAnnotateImages operation. |
|  [Schema$OutputConfig](./vision.vision_v1p1beta1.schema_outputconfig.md) | The desired output location and metadata. |
|  [Schema$Page](./vision.vision_v1p1beta1.schema_page.md) | Detected page from OCR. |
|  [Schema$Paragraph](./vision.vision_v1p1beta1.schema_paragraph.md) | Structural unit of text representing a number of words in certain order. |
|  [Schema$Position](./vision.vision_v1p1beta1.schema_position.md) | A 3D position in the image, used primarily for Face detection landmarks. A valid Position must have both x and y coordinates. The position coordinates are in the same scale as the original image. |
|  [Schema$Product](./vision.vision_v1p1beta1.schema_product.md) | A Product contains ReferenceImages. |
|  [Schema$ProductSearchResults](./vision.vision_v1p1beta1.schema_productsearchresults.md) | Results for a product search request. |
|  [Schema$Property](./vision.vision_v1p1beta1.schema_property.md) | A <code>Property</code> consists of a user-supplied name/value pair. |
|  [Schema$ReferenceImage](./vision.vision_v1p1beta1.schema_referenceimage.md) | A <code>ReferenceImage</code> represents a product image and its associated metadata, such as bounding boxes. |
|  [Schema$Result](./vision.vision_v1p1beta1.schema_result.md) | Information about a product. |
|  [Schema$SafeSearchAnnotation](./vision.vision_v1p1beta1.schema_safesearchannotation.md) | Set of features pertaining to the image, computed by computer vision methods over safe-search verticals (for example, adult, spoof, medical, violence). |
|  [Schema$Status](./vision.vision_v1p1beta1.schema_status.md) | The <code>Status</code> type defines a logical error model that is suitable for different programming environments, including REST APIs and RPC APIs. It is used by \[gRPC\](https://github.com/grpc). Each <code>Status</code> message contains three pieces of data: error code, error message, and error details.<!-- -->You can find out more about this error model and how to work with it in the \[API Design Guide\](https://cloud.google.com/apis/design/errors). |
|  [Schema$Symbol](./vision.vision_v1p1beta1.schema_symbol.md) | A single symbol representation. |
|  [Schema$TextAnnotation](./vision.vision_v1p1beta1.schema_textannotation.md) | TextAnnotation contains a structured representation of OCR extracted text. The hierarchy of an OCR extracted text structure is like this: TextAnnotation -<!-- -->&gt; Page -<!-- -->&gt; Block -<!-- -->&gt; Paragraph -<!-- -->&gt; Word -<!-- -->&gt; Symbol Each structural component, starting from Page, may further have their own properties. Properties describe detected languages, breaks etc.. Please refer to the TextAnnotation.TextProperty message definition below for more detail. |
|  [Schema$TextProperty](./vision.vision_v1p1beta1.schema_textproperty.md) | Additional information detected on the structural component. |
|  [Schema$Vertex](./vision.vision_v1p1beta1.schema_vertex.md) | A vertex represents a 2D point in the image. NOTE: the vertex coordinates are in the same scale as the original image. |
|  [Schema$WebDetection](./vision.vision_v1p1beta1.schema_webdetection.md) | Relevant information for the image from the Internet. |
|  [Schema$WebEntity](./vision.vision_v1p1beta1.schema_webentity.md) | Entity deduced from similar images on the Internet. |
|  [Schema$WebImage](./vision.vision_v1p1beta1.schema_webimage.md) | Metadata for online images. |
|  [Schema$WebLabel](./vision.vision_v1p1beta1.schema_weblabel.md) | Label to provide extra metadata for the web detection. |
|  [Schema$WebPage](./vision.vision_v1p1beta1.schema_webpage.md) | Metadata for web pages. |
|  [Schema$Word](./vision.vision_v1p1beta1.schema_word.md) | A word representation. |
|  [StandardParameters](./vision.vision_v1p1beta1.standardparameters.md) |  |

